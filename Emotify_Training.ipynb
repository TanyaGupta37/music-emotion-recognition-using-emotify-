{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanyaGupta37/music-emotion-recognition-using-emotify-/blob/main/Emotify_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYz5XTelICpE",
        "outputId": "a1c9ed03-468f-49db-b094-82cfa7e316b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeDI7eB7C982",
        "outputId": "75aefa26-8839-4574-ed8c-d44176b21b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking your Google Drive structure...\n",
            "\n",
            "1. Root of Drive:\n",
            "Colab Notebooks\n",
            "emotify_colab\n",
            "\n",
            "2. Looking for emotify_colab folder:\n",
            "emotify_colab\n",
            "\n",
            "3. If emotify_colab exists, checking inside:\n",
            "/content/drive/MyDrive/emotify_colab/:\n",
            "code  data  results\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/code:\n",
            "dl_models.py\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/data:\n",
            "audio\t\t      emotify_labels_5class.csv\n",
            "emotify_features.csv  emotify_labels_matched.csv\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/data/audio:\n",
            "emotify\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/data/audio/emotify:\n",
            "classical  electronic  pop  rock\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/data/audio/emotify/classical:\n",
            "100.mp3  19.mp3  28.mp3  37.mp3  46.mp3  55.mp3  64.mp3  73.mp3  82.mp3  91.mp3\n",
            "10.mp3\t 1.mp3\t 29.mp3  38.mp3  47.mp3  56.mp3  65.mp3  74.mp3  83.mp3  92.mp3\n",
            "11.mp3\t 20.mp3  2.mp3\t 39.mp3  48.mp3  57.mp3  66.mp3  75.mp3  84.mp3  93.mp3\n",
            "12.mp3\t 21.mp3  30.mp3  3.mp3\t 49.mp3  58.mp3  67.mp3  76.mp3  85.mp3  94.mp3\n",
            "13.mp3\t 22.mp3  31.mp3  40.mp3  4.mp3\t 59.mp3  68.mp3  77.mp3  86.mp3  95.mp3\n",
            "14.mp3\t 23.mp3  32.mp3  41.mp3  50.mp3  5.mp3\t 69.mp3  78.mp3  87.mp3  96.mp3\n",
            "15.mp3\t 24.mp3  33.mp3  42.mp3  51.mp3  60.mp3  6.mp3\t 79.mp3  88.mp3  97.mp3\n",
            "16.mp3\t 25.mp3  34.mp3  43.mp3  52.mp3  61.mp3  70.mp3  7.mp3\t 89.mp3  98.mp3\n",
            "17.mp3\t 26.mp3  35.mp3  44.mp3  53.mp3  62.mp3  71.mp3  80.mp3  8.mp3\t 99.mp3\n",
            "18.mp3\t 27.mp3  36.mp3  45.mp3  54.mp3  63.mp3  72.mp3  81.mp3  90.mp3  9.mp3\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/data/audio/emotify/electronic:\n",
            "100.mp3  19.mp3  28.mp3  37.mp3  46.mp3  55.mp3  64.mp3  73.mp3  82.mp3  91.mp3\n",
            "10.mp3\t 1.mp3\t 29.mp3  38.mp3  47.mp3  56.mp3  65.mp3  74.mp3  83.mp3  92.mp3\n",
            "11.mp3\t 20.mp3  2.mp3\t 39.mp3  48.mp3  57.mp3  66.mp3  75.mp3  84.mp3  93.mp3\n",
            "12.mp3\t 21.mp3  30.mp3  3.mp3\t 49.mp3  58.mp3  67.mp3  76.mp3  85.mp3  94.mp3\n",
            "13.mp3\t 22.mp3  31.mp3  40.mp3  4.mp3\t 59.mp3  68.mp3  77.mp3  86.mp3  95.mp3\n",
            "14.mp3\t 23.mp3  32.mp3  41.mp3  50.mp3  5.mp3\t 69.mp3  78.mp3  87.mp3  96.mp3\n",
            "15.mp3\t 24.mp3  33.mp3  42.mp3  51.mp3  60.mp3  6.mp3\t 79.mp3  88.mp3  97.mp3\n",
            "16.mp3\t 25.mp3  34.mp3  43.mp3  52.mp3  61.mp3  70.mp3  7.mp3\t 89.mp3  98.mp3\n",
            "17.mp3\t 26.mp3  35.mp3  44.mp3  53.mp3  62.mp3  71.mp3  80.mp3  8.mp3\t 99.mp3\n",
            "18.mp3\t 27.mp3  36.mp3  45.mp3  54.mp3  63.mp3  72.mp3  81.mp3  90.mp3  9.mp3\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/data/audio/emotify/pop:\n",
            "100.mp3  19.mp3  28.mp3  37.mp3  46.mp3  55.mp3  64.mp3  73.mp3  82.mp3  91.mp3\n",
            "10.mp3\t 1.mp3\t 29.mp3  38.mp3  47.mp3  56.mp3  65.mp3  74.mp3  83.mp3  92.mp3\n",
            "11.mp3\t 20.mp3  2.mp3\t 39.mp3  48.mp3  57.mp3  66.mp3  75.mp3  84.mp3  93.mp3\n",
            "12.mp3\t 21.mp3  30.mp3  3.mp3\t 49.mp3  58.mp3  67.mp3  76.mp3  85.mp3  94.mp3\n",
            "13.mp3\t 22.mp3  31.mp3  40.mp3  4.mp3\t 59.mp3  68.mp3  77.mp3  86.mp3  95.mp3\n",
            "14.mp3\t 23.mp3  32.mp3  41.mp3  50.mp3  5.mp3\t 69.mp3  78.mp3  87.mp3  96.mp3\n",
            "15.mp3\t 24.mp3  33.mp3  42.mp3  51.mp3  60.mp3  6.mp3\t 79.mp3  88.mp3  97.mp3\n",
            "16.mp3\t 25.mp3  34.mp3  43.mp3  52.mp3  61.mp3  70.mp3  7.mp3\t 89.mp3  98.mp3\n",
            "17.mp3\t 26.mp3  35.mp3  44.mp3  53.mp3  62.mp3  71.mp3  80.mp3  8.mp3\t 99.mp3\n",
            "18.mp3\t 27.mp3  36.mp3  45.mp3  54.mp3  63.mp3  72.mp3  81.mp3  90.mp3  9.mp3\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/data/audio/emotify/rock:\n",
            "100.mp3  19.mp3  28.mp3  37.mp3  46.mp3  55.mp3  64.mp3  73.mp3  82.mp3  91.mp3\n",
            "10.mp3\t 1.mp3\t 29.mp3  38.mp3  47.mp3  56.mp3  65.mp3  74.mp3  83.mp3  92.mp3\n",
            "11.mp3\t 20.mp3  2.mp3\t 39.mp3  48.mp3  57.mp3  66.mp3  75.mp3  84.mp3  93.mp3\n",
            "12.mp3\t 21.mp3  30.mp3  3.mp3\t 49.mp3  58.mp3  67.mp3  76.mp3  85.mp3  94.mp3\n",
            "13.mp3\t 22.mp3  31.mp3  40.mp3  4.mp3\t 59.mp3  68.mp3  77.mp3  86.mp3  95.mp3\n",
            "14.mp3\t 23.mp3  32.mp3  41.mp3  50.mp3  5.mp3\t 69.mp3  78.mp3  87.mp3  96.mp3\n",
            "15.mp3\t 24.mp3  33.mp3  42.mp3  51.mp3  60.mp3  6.mp3\t 79.mp3  88.mp3  97.mp3\n",
            "16.mp3\t 25.mp3  34.mp3  43.mp3  52.mp3  61.mp3  70.mp3  7.mp3\t 89.mp3  98.mp3\n",
            "17.mp3\t 26.mp3  35.mp3  44.mp3  53.mp3  62.mp3  71.mp3  80.mp3  8.mp3\t 99.mp3\n",
            "18.mp3\t 27.mp3  36.mp3  45.mp3  54.mp3  63.mp3  72.mp3  81.mp3  90.mp3  9.mp3\n",
            "\n",
            "/content/drive/MyDrive/emotify_colab/results:\n",
            "dl_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Check what's in your Drive\n",
        "import os\n",
        "\n",
        "print(\"Checking your Google Drive structure...\")\n",
        "print(\"\\n1. Root of Drive:\")\n",
        "!ls /content/drive/MyDrive/ | head -20\n",
        "\n",
        "print(\"\\n2. Looking for emotify_colab folder:\")\n",
        "!ls /content/drive/MyDrive/ | grep emotify\n",
        "\n",
        "print(\"\\n3. If emotify_colab exists, checking inside:\")\n",
        "if os.path.exists('/content/drive/MyDrive/emotify_colab'):\n",
        "    !ls -R /content/drive/MyDrive/emotify_colab/\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è emotify_colab folder NOT FOUND in Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LklZymhSIT4m",
        "outputId": "88000e9d-3b60-4648-9c69-ad8e07c1aebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Files copied from Drive!\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Copy files from Drive\n",
        "\n",
        "import os\n",
        "\n",
        "# Create directories first\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('code', exist_ok=True)\n",
        "\n",
        "!cp /content/drive/MyDrive/emotify_colab/data/emotify_features.csv data/\n",
        "!cp /content/drive/MyDrive/emotify_colab/data/emotify_labels_matched.csv data/\n",
        "!cp /content/drive/MyDrive/emotify_colab/code/dl_models.py code/\n",
        "\n",
        "print(\"‚úì Files copied from Drive!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANZobK_GJwXL",
        "outputId": "c6a0bc17-906c-4674-aaef-fc593c861dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Packages installed!\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Install packages\n",
        "!pip install -q scikit-learn pandas numpy tensorflow\n",
        "print(\"‚úì Packages installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSAB2ltcJ1t3",
        "outputId": "45390b09-ac7b-4536-cb52-67d54f7d87ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DATA LOADED\n",
            "============================================================\n",
            "Features shape: (399, 150)\n",
            "Labels shape: (399, 2)\n",
            "\n",
            "Emotion distribution:\n",
            "emotion\n",
            "relaxing      69\n",
            "anxious       54\n",
            "amusing       49\n",
            "happy         41\n",
            "energizing    38\n",
            "annoying      36\n",
            "dreamy        35\n",
            "sad           30\n",
            "neutral       27\n",
            "joyful        20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "X shape: (399, 149)\n",
            "y shape: (399,)\n",
            "Unique classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Load and verify data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "features_df = pd.read_csv('data/emotify_features.csv')\n",
        "labels_df = pd.read_csv('data/emotify_labels_matched.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA LOADED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Labels shape: {labels_df.shape}\")\n",
        "print(f\"\\nEmotion distribution:\")\n",
        "print(features_df['emotion'].value_counts())\n",
        "\n",
        "# Prepare data\n",
        "X = features_df.drop('emotion', axis=1).values\n",
        "y = features_df['emotion'].values\n",
        "\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Unique classes: {len(np.unique(y))}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Encode labels if needed\n",
        "# ------------------------------------------------------------\n",
        "if y.dtype == object:\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "else:\n",
        "    y_enc = y\n",
        "\n",
        "num_classes = len(np.unique(y_enc))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Models (simple, like earlier pipeline)\n",
        "# ------------------------------------------------------------\n",
        "knn_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=11, weights='distance'))\n",
        "])\n",
        "\n",
        "svm_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(\n",
        "        kernel='rbf',\n",
        "        C=10,\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        class_weight='balanced'\n",
        "    ))\n",
        "])\n",
        "\n",
        "models = {\n",
        "    \"KNN\": knn_model,\n",
        "    \"SVM\": svm_model\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Evaluation (same style as earlier models)\n",
        "# ------------------------------------------------------------\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"TRAINING {name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    accs, aucs, f1s = [], [], []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y_enc), 1):\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y_enc[train_idx], y_enc[val_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_proba = model.predict_proba(X_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average='macro')\n",
        "\n",
        "        y_val_bin = label_binarize(y_val, classes=range(num_classes))\n",
        "        auc = roc_auc_score(\n",
        "            y_val_bin,\n",
        "            y_proba,\n",
        "            average='macro',\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "        accs.append(acc)\n",
        "        f1s.append(f1)\n",
        "        aucs.append(auc)\n",
        "\n",
        "        print(\n",
        "            f\"Fold {fold:2d} | \"\n",
        "            f\"Acc: {acc:.4f} | \"\n",
        "            f\"AUC: {auc:.4f} | \"\n",
        "            f\"F1: {f1:.4f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*60)\n",
        "    print(f\"{name} FINAL RESULTS\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"Accuracy:  {np.mean(accs):.4f} ¬± {np.std(accs):.4f}\")\n",
        "    print(f\"ROC-AUC:   {np.mean(aucs):.4f} ¬± {np.std(aucs):.4f}\")\n",
        "    print(f\"F1-Score:  {np.mean(f1s):.4f} ¬± {np.std(f1s):.4f}\")\n",
        "\n",
        "    results[name] = {\n",
        "        'accuracy_mean': np.mean(accs),\n",
        "        'accuracy_std': np.std(accs),\n",
        "        'auc_mean': np.mean(aucs),\n",
        "        'auc_std': np.std(aucs),\n",
        "        'f1_mean': np.mean(f1s),\n",
        "        'f1_std': np.std(f1s),\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Summary (matches earlier benchmark style)\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BASELINE RESULTS (EARLIER PIPELINE)\")\n",
        "print(\"=\"*60)\n",
        "for model, res in results.items():\n",
        "    print(\n",
        "        f\"{model:<4} | \"\n",
        "        f\"AUC: {res['auc_mean']:.4f} ¬± {res['auc_std']:.4f} | \"\n",
        "        f\"F1: {res['f1_mean']:.4f} ¬± {res['f1_std']:.4f}\"\n",
        "    )\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQakVDI-tz-E",
        "outputId": "601febfb-fb82-41d8-fd4d-2e6b24aa2985"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING KNN\n",
            "============================================================\n",
            "Fold  1 | Acc: 0.1250 | AUC: 0.4981 | F1: 0.1111\n",
            "Fold  2 | Acc: 0.2000 | AUC: 0.5613 | F1: 0.1298\n",
            "Fold  3 | Acc: 0.2500 | AUC: 0.5473 | F1: 0.1578\n",
            "Fold  4 | Acc: 0.1250 | AUC: 0.5086 | F1: 0.0711\n",
            "Fold  5 | Acc: 0.1250 | AUC: 0.5109 | F1: 0.0994\n",
            "Fold  6 | Acc: 0.2000 | AUC: 0.5884 | F1: 0.1305\n",
            "Fold  7 | Acc: 0.0250 | AUC: 0.5040 | F1: 0.0125\n",
            "Fold  8 | Acc: 0.2000 | AUC: 0.5600 | F1: 0.1640\n",
            "Fold  9 | Acc: 0.1500 | AUC: 0.5740 | F1: 0.1405\n",
            "Fold 10 | Acc: 0.1282 | AUC: 0.5409 | F1: 0.1051\n",
            "------------------------------------------------------------\n",
            "KNN FINAL RESULTS\n",
            "------------------------------------------------------------\n",
            "Accuracy:  0.1528 ¬± 0.0595\n",
            "ROC-AUC:   0.5394 ¬± 0.0305\n",
            "F1-Score:  0.1122 ¬± 0.0425\n",
            "\n",
            "============================================================\n",
            "TRAINING SVM\n",
            "============================================================\n",
            "Fold  1 | Acc: 0.0750 | AUC: 0.6186 | F1: 0.0448\n",
            "Fold  2 | Acc: 0.1750 | AUC: 0.5520 | F1: 0.1392\n",
            "Fold  3 | Acc: 0.0500 | AUC: 0.5673 | F1: 0.0251\n",
            "Fold  4 | Acc: 0.1750 | AUC: 0.5638 | F1: 0.0920\n",
            "Fold  5 | Acc: 0.1500 | AUC: 0.5765 | F1: 0.1150\n",
            "Fold  6 | Acc: 0.1250 | AUC: 0.5583 | F1: 0.1275\n",
            "Fold  7 | Acc: 0.1000 | AUC: 0.5633 | F1: 0.0911\n",
            "Fold  8 | Acc: 0.1250 | AUC: 0.5952 | F1: 0.0821\n",
            "Fold  9 | Acc: 0.2250 | AUC: 0.6408 | F1: 0.1991\n",
            "Fold 10 | Acc: 0.1538 | AUC: 0.5278 | F1: 0.0785\n",
            "------------------------------------------------------------\n",
            "SVM FINAL RESULTS\n",
            "------------------------------------------------------------\n",
            "Accuracy:  0.1354 ¬± 0.0491\n",
            "ROC-AUC:   0.5764 ¬± 0.0316\n",
            "F1-Score:  0.0994 ¬± 0.0468\n",
            "\n",
            "============================================================\n",
            "BASELINE RESULTS (EARLIER PIPELINE)\n",
            "============================================================\n",
            "KNN  | AUC: 0.5394 ¬± 0.0305 | F1: 0.1122 ¬± 0.0425\n",
            "SVM  | AUC: 0.5764 ¬± 0.0316 | F1: 0.0994 ¬± 0.0468\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62wk6qZiJ-OQ",
        "outputId": "db3683a3-acd0-4dca-8e98-61605c779fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "\n",
            "üöÄ Training Model 1/2...\n",
            "\n",
            "============================================================\n",
            "Training MLP (3-layer)\n",
            "============================================================\n",
            "\n",
            "Fold 1/10...\n",
            "  Acc: 0.1500, AUC: 0.5323, F1: 0.0731\n",
            "\n",
            "Fold 2/10...\n",
            "  Acc: 0.2500, AUC: 0.5774, F1: 0.1031\n",
            "\n",
            "Fold 3/10...\n",
            "  Acc: 0.2000, AUC: 0.5880, F1: 0.0667\n",
            "\n",
            "Fold 4/10...\n",
            "  Acc: 0.1250, AUC: 0.5514, F1: 0.0707\n",
            "\n",
            "Fold 5/10...\n",
            "  Acc: 0.1750, AUC: 0.5965, F1: 0.0724\n",
            "\n",
            "Fold 6/10...\n",
            "  Acc: 0.1000, AUC: 0.5344, F1: 0.0468\n",
            "\n",
            "Fold 7/10...\n",
            "  Acc: 0.1000, AUC: 0.5469, F1: 0.0342\n",
            "\n",
            "Fold 8/10...\n",
            "  Acc: 0.2750, AUC: 0.6205, F1: 0.1102\n",
            "\n",
            "Fold 9/10...\n",
            "  Acc: 0.1750, AUC: 0.6151, F1: 0.0799\n",
            "\n",
            "Fold 10/10...\n",
            "  Acc: 0.1282, AUC: 0.6551, F1: 0.0294\n",
            "\n",
            "============================================================\n",
            "MLP (3-layer) - FINAL RESULTS\n",
            "============================================================\n",
            "Accuracy:  0.1678 ¬± 0.0568\n",
            "ROC-AUC:   0.5818 ¬± 0.0389\n",
            "F1-Score:  0.0686 ¬± 0.0251\n",
            "\n",
            "üöÄ Training Model 2/2...\n",
            "\n",
            "============================================================\n",
            "Training Deep MLP (5-layer + BatchNorm)\n",
            "============================================================\n",
            "\n",
            "Fold 1/10...\n",
            "  Acc: 0.0750, AUC: 0.5497, F1: 0.0504\n",
            "\n",
            "Fold 2/10...\n",
            "  Acc: 0.2000, AUC: 0.5650, F1: 0.1320\n",
            "\n",
            "Fold 3/10...\n",
            "  Acc: 0.2000, AUC: 0.5343, F1: 0.1154\n",
            "\n",
            "Fold 4/10...\n",
            "  Acc: 0.2000, AUC: 0.4482, F1: 0.0828\n",
            "\n",
            "Fold 5/10...\n",
            "  Acc: 0.1750, AUC: 0.5876, F1: 0.0866\n",
            "\n",
            "Fold 6/10...\n",
            "  Acc: 0.2250, AUC: 0.5877, F1: 0.1844\n",
            "\n",
            "Fold 7/10...\n",
            "  Acc: 0.1750, AUC: 0.5514, F1: 0.1361\n",
            "\n",
            "Fold 8/10...\n",
            "  Acc: 0.1500, AUC: 0.5295, F1: 0.0854\n",
            "\n",
            "Fold 9/10...\n",
            "  Acc: 0.1750, AUC: 0.6412, F1: 0.0865\n",
            "\n",
            "Fold 10/10...\n",
            "  Acc: 0.1538, AUC: 0.6060, F1: 0.0801\n",
            "\n",
            "============================================================\n",
            "Deep MLP (5-layer + BatchNorm) - FINAL RESULTS\n",
            "============================================================\n",
            "Accuracy:  0.1729 ¬± 0.0392\n",
            "ROC-AUC:   0.5601 ¬± 0.0495\n",
            "F1-Score:  0.1040 ¬± 0.0364\n",
            "\n",
            "============================================================\n",
            "COMPARISON WITH BASELINE\n",
            "============================================================\n",
            "Baseline (KNN):  0.5394 ROC-AUC\n",
            "Baseline (SVM):  0.5716 ROC-AUC\n",
            "Best Baseline:   0.5716 ROC-AUC\n",
            "\n",
            "Your MLP:        0.5818 ¬± 0.0389 ROC-AUC\n",
            "Your Deep MLP:   0.5601 ¬± 0.0495 ROC-AUC\n",
            "\n",
            "üéâ MLP improvement over best baseline: +1.78%\n",
            "============================================================\n",
            "\n",
            "‚úì Results saved to dl_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Import models and train\n",
        "import sys\n",
        "sys.path.append('code')\n",
        "from dl_models import build_mlp_model, build_deeper_mlp, evaluate_model\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train MLP (3-layer)\n",
        "print(\"\\nüöÄ Training Model 1/2...\")\n",
        "results_mlp = evaluate_model(\n",
        "    build_mlp_model,\n",
        "    X, y,\n",
        "    \"MLP (3-layer)\",\n",
        "    n_splits=10\n",
        ")\n",
        "\n",
        "# Train Deeper MLP (5-layer with BatchNorm)\n",
        "print(\"\\nüöÄ Training Model 2/2...\")\n",
        "results_deep_mlp = evaluate_model(\n",
        "    build_deeper_mlp,\n",
        "    X, y,\n",
        "    \"Deep MLP (5-layer + BatchNorm)\",\n",
        "    n_splits=10\n",
        ")\n",
        "\n",
        "# Summary\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame([results_mlp, results_deep_mlp])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON WITH BASELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_knn = 0.5394\n",
        "baseline_svm = 0.5716\n",
        "baseline_best = max(baseline_knn, baseline_svm)\n",
        "\n",
        "print(f\"Baseline (KNN):  {baseline_knn:.4f} ROC-AUC\")\n",
        "print(f\"Baseline (SVM):  {baseline_svm:.4f} ROC-AUC\")\n",
        "print(f\"Best Baseline:   {baseline_best:.4f} ROC-AUC\")\n",
        "\n",
        "print(f\"\\nYour MLP:        {results_mlp['auc_mean']:.4f} ¬± {results_mlp['auc_std']:.4f} ROC-AUC\")\n",
        "print(f\"Your Deep MLP:   {results_deep_mlp['auc_mean']:.4f} ¬± {results_deep_mlp['auc_std']:.4f} ROC-AUC\")\n",
        "\n",
        "# Improvement checks\n",
        "if results_mlp['auc_mean'] > baseline_best:\n",
        "    improvement = ((results_mlp['auc_mean'] - baseline_best) / baseline_best) * 100\n",
        "    print(f\"\\nüéâ MLP improvement over best baseline: +{improvement:.2f}%\")\n",
        "\n",
        "if results_deep_mlp['auc_mean'] > baseline_best:\n",
        "    improvement = ((results_deep_mlp['auc_mean'] - baseline_best) / baseline_best) * 100\n",
        "    print(f\"üéâ Deep MLP improvement over best baseline: +{improvement:.2f}%\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('dl_results.csv', index=False)\n",
        "results_df.to_csv('/content/drive/MyDrive/emotify_colab/results/dl_results.csv', index=False)\n",
        "print(\"\\n‚úì Results saved to dl_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCGTm5IIEYGl"
      },
      "outputs": [],
      "source": [
        " from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LvKlIbDjujU",
        "outputId": "dfa1123b-54ec-49a7-ce45-308937364305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Random Forest\n",
            "RF ROC-AUC: 0.5120 ¬± 0.0419\n",
            "RF F1:      0.0953\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "print(\"\\n Training Random Forest\")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "rf_aucs, rf_f1s = [], []\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "classes = np.unique(y_enc)\n",
        "\n",
        "for train_idx, test_idx in skf.split(X, y_enc):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
        "\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_proba = rf.predict_proba(X_test)\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "    y_test_bin = label_binarize(y_test, classes=classes)\n",
        "\n",
        "    rf_aucs.append(\n",
        "        roc_auc_score(y_test_bin, y_proba, average='macro', multi_class='ovr')\n",
        "    )\n",
        "    rf_f1s.append(\n",
        "        f1_score(y_test, y_pred, average='macro')\n",
        "    )\n",
        "\n",
        "print(f\"RF ROC-AUC: {np.mean(rf_aucs):.4f} ¬± {np.std(rf_aucs):.4f}\")\n",
        "print(f\"RF F1:      {np.mean(rf_f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nPR79xkj9WR",
        "outputId": "8128553d-15b8-4318-c835-06c5ca1659af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Extra Trees\n",
            "ET ROC-AUC: 0.5434 ¬± 0.0520\n",
            "ET F1:      0.0934\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Training Extra Trees\")\n",
        "\n",
        "et = ExtraTreesClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "et_aucs, et_f1s = [], []\n",
        "\n",
        "for train_idx, test_idx in skf.split(X, y_enc):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
        "\n",
        "    et.fit(X_train, y_train)\n",
        "    y_proba = et.predict_proba(X_test)\n",
        "    y_pred = et.predict(X_test)\n",
        "\n",
        "    y_test_bin = label_binarize(y_test, classes=classes)\n",
        "\n",
        "    et_aucs.append(\n",
        "        roc_auc_score(y_test_bin, y_proba, average='macro', multi_class='ovr')\n",
        "    )\n",
        "    et_f1s.append(\n",
        "        f1_score(y_test, y_pred, average='macro')\n",
        "    )\n",
        "\n",
        "print(f\"ET ROC-AUC: {np.mean(et_aucs):.4f} ¬± {np.std(et_aucs):.4f}\")\n",
        "print(f\"ET F1:      {np.mean(et_f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j48pDyoOmeU9",
        "outputId": "293c2c32-1d93-4dd6-d44e-34d25715a244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úì CatBoost installed\n"
          ]
        }
      ],
      "source": [
        "#Installing CatBoost\n",
        "!pip install -q catboost\n",
        "print(\"‚úì CatBoost installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeAFP4EJnxbg"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llBeAVfLn4F7",
        "outputId": "6ac8fece-250b-4d96-ab77-b202f9709d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training CatBoost\n",
            "Fold  1 | AUC: 0.4764 | F1: 0.0516\n",
            "Fold  2 | AUC: 0.5527 | F1: 0.0916\n",
            "Fold  3 | AUC: 0.5075 | F1: 0.0980\n",
            "Fold  4 | AUC: 0.5025 | F1: 0.1288\n",
            "Fold  5 | AUC: 0.5485 | F1: 0.1036\n",
            "Fold  6 | AUC: 0.5041 | F1: 0.1062\n",
            "Fold  7 | AUC: 0.5171 | F1: 0.0976\n",
            "Fold  8 | AUC: 0.6587 | F1: 0.1591\n",
            "Fold  9 | AUC: 0.5493 | F1: 0.2229\n",
            "Fold 10 | AUC: 0.6128 | F1: 0.1313\n",
            "\n",
            "------------------------------------------------------------\n",
            "CATBOOST FINAL RESULTS\n",
            "------------------------------------------------------------\n",
            "ROC-AUC: 0.5430 ¬± 0.0529\n",
            "F1:      0.1191\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Training CatBoost\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "classes = np.unique(y_enc)\n",
        "\n",
        "# 10-fold CV\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "cb_aucs, cb_f1s = [], []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y_enc), 1):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=500,\n",
        "        depth=6,\n",
        "        learning_rate=0.05,\n",
        "        loss_function='MultiClass',\n",
        "        eval_metric='MultiClass',\n",
        "        verbose=False,\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_proba = model.predict_proba(X_test)\n",
        "    y_pred = model.predict(X_test).ravel()\n",
        "\n",
        "    y_test_bin = label_binarize(y_test, classes=classes)\n",
        "\n",
        "    auc = roc_auc_score(\n",
        "        y_test_bin,\n",
        "        y_proba,\n",
        "        average='macro',\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    cb_aucs.append(auc)\n",
        "    cb_f1s.append(f1)\n",
        "\n",
        "    print(\n",
        "        f\"Fold {fold:2d} | \"\n",
        "        f\"AUC: {auc:.4f} | \"\n",
        "        f\"F1: {f1:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"CATBOOST FINAL RESULTS\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"ROC-AUC: {np.mean(cb_aucs):.4f} ¬± {np.std(cb_aucs):.4f}\")\n",
        "print(f\"F1:      {np.mean(cb_f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMzFjlbfn9zB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxQ-O7A4tK5u",
        "outputId": "645864c6-de33-4ccf-c742-759302ea17f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Logistic Regression\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold  1 | AUC: 0.5106 | F1: 0.0786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold  2 | AUC: 0.5387 | F1: 0.1631\n",
            "Fold  3 | AUC: 0.4721 | F1: 0.0167\n",
            "Fold  4 | AUC: 0.4085 | F1: 0.1137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold  5 | AUC: 0.5236 | F1: 0.0843\n",
            "Fold  6 | AUC: 0.5248 | F1: 0.1308\n",
            "Fold  7 | AUC: 0.5540 | F1: 0.0984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold  8 | AUC: 0.4892 | F1: 0.0582\n",
            "Fold  9 | AUC: 0.5595 | F1: 0.1041\n",
            "Fold 10 | AUC: 0.5327 | F1: 0.0652\n",
            "\n",
            "------------------------------------------------------------\n",
            "LOGISTIC REGRESSION FINAL RESULTS\n",
            "------------------------------------------------------------\n",
            "ROC-AUC: 0.5114 ¬± 0.0427\n",
            "F1:      0.0913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Logistic Regression\n",
        "\n",
        "print(\"\\n Training Logistic Regression\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "classes = np.unique(y_enc)\n",
        "\n",
        "# Scale features (important for LR)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "lr_aucs, lr_f1s = [], []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X_scaled, y_enc), 1):\n",
        "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "    y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
        "\n",
        "    lr = LogisticRegression(\n",
        "        max_iter=2000,\n",
        "        multi_class='multinomial',\n",
        "        solver='lbfgs',\n",
        "        class_weight='balanced',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    lr.fit(X_train, y_train)\n",
        "\n",
        "    y_proba = lr.predict_proba(X_test)\n",
        "    y_pred = lr.predict(X_test)\n",
        "\n",
        "    y_test_bin = label_binarize(y_test, classes=classes)\n",
        "\n",
        "    auc = roc_auc_score(\n",
        "        y_test_bin,\n",
        "        y_proba,\n",
        "        average='macro',\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    lr_aucs.append(auc)\n",
        "    lr_f1s.append(f1)\n",
        "\n",
        "    print(f\"Fold {fold:2d} | AUC: {auc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"LOGISTIC REGRESSION FINAL RESULTS\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"ROC-AUC: {np.mean(lr_aucs):.4f} ¬± {np.std(lr_aucs):.4f}\")\n",
        "print(f\"F1:      {np.mean(lr_f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgSdXedztRYe"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO5930GStuDh",
        "outputId": "592ee980-7e7a-4cbd-8dad-e4b5f605a38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Naive Bayes (Gaussian)\n",
            "Fold  1 | AUC: 0.5542 | F1: 0.1600\n",
            "Fold  2 | AUC: 0.4262 | F1: 0.0615\n",
            "Fold  3 | AUC: 0.5145 | F1: 0.1404\n",
            "Fold  4 | AUC: 0.5198 | F1: 0.0962\n",
            "Fold  5 | AUC: 0.5170 | F1: 0.1190\n",
            "Fold  6 | AUC: 0.5022 | F1: 0.0404\n",
            "Fold  7 | AUC: 0.4518 | F1: 0.0487\n",
            "Fold  8 | AUC: 0.5692 | F1: 0.0782\n",
            "Fold  9 | AUC: 0.5278 | F1: 0.0694\n",
            "Fold 10 | AUC: 0.5538 | F1: 0.0957\n",
            "\n",
            "------------------------------------------------------------\n",
            "NAIVE BAYES FINAL RESULTS\n",
            "------------------------------------------------------------\n",
            "ROC-AUC: 0.5137 ¬± 0.0427\n",
            "F1:      0.0910\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Training Naive Bayes (Gaussian)\")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "nb_aucs, nb_f1s = [], []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X_scaled, y_enc), 1):\n",
        "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "    y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
        "\n",
        "    nb = GaussianNB()\n",
        "    nb.fit(X_train, y_train)\n",
        "\n",
        "    y_proba = nb.predict_proba(X_test)\n",
        "    y_pred = nb.predict(X_test)\n",
        "\n",
        "    y_test_bin = label_binarize(y_test, classes=classes)\n",
        "\n",
        "    auc = roc_auc_score(\n",
        "        y_test_bin,\n",
        "        y_proba,\n",
        "        average='macro',\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    nb_aucs.append(auc)\n",
        "    nb_f1s.append(f1)\n",
        "\n",
        "    print(f\"Fold {fold:2d} | AUC: {auc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"NAIVE BAYES FINAL RESULTS\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"ROC-AUC: {np.mean(nb_aucs):.4f} ¬± {np.std(nb_aucs):.4f}\")\n",
        "print(f\"F1:      {np.mean(nb_f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AJSDHvHvuzW"
      },
      "source": [
        "# Deep Learning Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5PrwdtYtzQK",
        "outputId": "075bb11b-f6e1-425b-c815-d5b034f21164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Audio libraries installed\n"
          ]
        }
      ],
      "source": [
        "!pip install -q librosa soundfile\n",
        "print(\"‚úì Audio libraries installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT8aTVxONsP5",
        "outputId": "c20e2dbf-baf9-4b0f-969f-08f3dd580789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code  data  results\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/emotify_colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJaopbd4N9A1",
        "outputId": "65764505-831a-4a14-8d01-0288653c0827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio  emotify_features.csv  emotify_labels_matched.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/emotify_colab/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MFqqQIfOCYn",
        "outputId": "e320556e-0f9b-467b-ac40-4eea14920232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.mp3  19.mp3  28.mp3  37.mp3  46.mp3  55.mp3  64.mp3  73.mp3  82.mp3  91.mp3\n",
            "10.mp3\t 1.mp3\t 29.mp3  38.mp3  47.mp3  56.mp3  65.mp3  74.mp3  83.mp3  92.mp3\n",
            "11.mp3\t 20.mp3  2.mp3\t 39.mp3  48.mp3  57.mp3  66.mp3  75.mp3  84.mp3  93.mp3\n",
            "12.mp3\t 21.mp3  30.mp3  3.mp3\t 49.mp3  58.mp3  67.mp3  76.mp3  85.mp3  94.mp3\n",
            "13.mp3\t 22.mp3  31.mp3  40.mp3  4.mp3\t 59.mp3  68.mp3  77.mp3  86.mp3  95.mp3\n",
            "14.mp3\t 23.mp3  32.mp3  41.mp3  50.mp3  5.mp3\t 69.mp3  78.mp3  87.mp3  96.mp3\n",
            "15.mp3\t 24.mp3  33.mp3  42.mp3  51.mp3  60.mp3  6.mp3\t 79.mp3  88.mp3  97.mp3\n",
            "16.mp3\t 25.mp3  34.mp3  43.mp3  52.mp3  61.mp3  70.mp3  7.mp3\t 89.mp3  98.mp3\n",
            "17.mp3\t 26.mp3  35.mp3  44.mp3  53.mp3  62.mp3  71.mp3  80.mp3  8.mp3\t 99.mp3\n",
            "18.mp3\t 27.mp3  36.mp3  45.mp3  54.mp3  63.mp3  72.mp3  81.mp3  90.mp3  9.mp3\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/emotify_colab/data/audio/emotify/classical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7a4nIxhM-ni",
        "outputId": "3c1f23df-c0c7-47c5-e344-e8a1c647fcff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio base exists: True\n",
            "Pop samples: ['3.mp3', '6.mp3', '9.mp3', '13.mp3', '11.mp3']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "audio_base = '/content/drive/MyDrive/emotify_colab/data/audio/emotify'\n",
        "\n",
        "print(\"Audio base exists:\", os.path.exists(audio_base))\n",
        "print(\"Pop samples:\", os.listdir(os.path.join(audio_base, 'pop'))[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3QidriX0o8f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Audio parameters\n",
        "SAMPLE_RATE = 22050\n",
        "N_MELS = 128\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "MAX_DURATION = 10  # seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPKIQ_hZ0uPR",
        "outputId": "ad327de4-c88f-4670-a99f-825f8c1da822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio base exists: True\n",
            "Genres: ['rock', 'pop', 'electronic', 'classical']\n",
            "Pop samples: ['3.mp3', '6.mp3', '9.mp3', '13.mp3', '11.mp3']\n"
          ]
        }
      ],
      "source": [
        "audio_base = '/content/drive/MyDrive/emotify_colab/data/audio/emotify'\n",
        "\n",
        "print(\"Audio base exists:\", os.path.exists(audio_base))\n",
        "print(\"Genres:\", os.listdir(audio_base))\n",
        "print(\"Pop samples:\", os.listdir(os.path.join(audio_base, 'pop'))[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5k-k3H40xbZ"
      },
      "outputs": [],
      "source": [
        "def augment_audio(y):\n",
        "    # Random volume gain\n",
        "    gain = np.random.uniform(0.8, 1.2)\n",
        "    y = y * gain\n",
        "\n",
        "    # Add small Gaussian noise\n",
        "    noise = np.random.randn(len(y)) * 0.005\n",
        "    y = y + noise\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def extract_logmel(audio_path, augment=False):\n",
        "    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=MAX_DURATION)\n",
        "\n",
        "    if augment:\n",
        "        y = augment_audio(y)\n",
        "\n",
        "    max_len = SAMPLE_RATE * MAX_DURATION\n",
        "    if len(y) < max_len:\n",
        "        y = np.pad(y, (0, max_len - len(y)))\n",
        "    else:\n",
        "        y = y[:max_len]\n",
        "\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_fft=N_FFT,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "    logmel = librosa.power_to_db(mel)\n",
        "    logmel = (logmel - np.mean(logmel)) / (np.std(logmel) + 1e-8)\n",
        "\n",
        "    return logmel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vloMlh_g1Cu5",
        "outputId": "f288c367-578d-4dd5-9706-c93c0871a183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 399/399 [04:03<00:00,  1.64it/s]\n"
          ]
        }
      ],
      "source": [
        "labels_df = pd.read_csv(\n",
        "    '/content/drive/MyDrive/emotify_colab/data/emotify_labels_matched.csv'\n",
        ")\n",
        "\n",
        "X_specs = []\n",
        "y_labels = []\n",
        "\n",
        "for _, row in tqdm(labels_df.iterrows(), total=len(labels_df)):\n",
        "    audio_path = os.path.join(audio_base, row['filename'])\n",
        "\n",
        "    if not os.path.exists(audio_path):\n",
        "        continue\n",
        "\n",
        "    spec = extract_logmel(audio_path, augment = True)\n",
        "    X_specs.append(spec)\n",
        "    y_labels.append(row['dominant_emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVsR2NCJQMV5",
        "outputId": "55d1bb4a-ce14-40a2-aa5f-733e62ebe2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (399, 128, 431, 1)\n",
            "y shape: (399,)\n",
            "Classes: 10\n"
          ]
        }
      ],
      "source": [
        " X = np.array(X_specs)[..., np.newaxis]  # (N, 128, T, 1)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_labels)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Classes:\", len(np.unique(y)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg82MIClXTZR"
      },
      "outputs": [],
      "source": [
        "# CNN Model (Baseline)\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "\n",
        "        layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StS3P1luXcRC",
        "outputId": "d42d1aa2-e521-4fd0-c8cf-f49090ed8963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model = build_cnn(X.shape[1:], len(np.unique(y)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFmomk7YXlhS",
        "outputId": "eac8dea3-04e1-4b5e-8afe-487d04c5d5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 377ms/step - accuracy: 0.0616 - loss: 2.3442 - val_accuracy: 0.1375 - val_loss: 2.3068\n",
            "Epoch 2/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1380 - loss: 2.2585 - val_accuracy: 0.1250 - val_loss: 2.3052\n",
            "Epoch 3/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0990 - loss: 2.2958 - val_accuracy: 0.1250 - val_loss: 2.3058\n",
            "Epoch 4/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1992 - loss: 2.2359 - val_accuracy: 0.1250 - val_loss: 2.3065\n",
            "Epoch 5/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1907 - loss: 2.2165 - val_accuracy: 0.1250 - val_loss: 2.3029\n",
            "Epoch 6/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.2108 - loss: 2.2399 - val_accuracy: 0.1250 - val_loss: 2.2977\n",
            "Epoch 7/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.1959 - loss: 2.1971 - val_accuracy: 0.1250 - val_loss: 2.2970\n",
            "Epoch 8/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.1930 - loss: 2.1914 - val_accuracy: 0.1250 - val_loss: 2.2939\n",
            "Epoch 9/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.1920 - loss: 2.2036 - val_accuracy: 0.1625 - val_loss: 2.2886\n",
            "Epoch 10/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2206 - loss: 2.1683 - val_accuracy: 0.1750 - val_loss: 2.2810\n",
            "Epoch 11/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2446 - loss: 2.1380 - val_accuracy: 0.1625 - val_loss: 2.2750\n",
            "Epoch 12/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2137 - loss: 2.1743 - val_accuracy: 0.1250 - val_loss: 2.2717\n",
            "Epoch 13/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2684 - loss: 2.1402 - val_accuracy: 0.1500 - val_loss: 2.2699\n",
            "Epoch 14/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2101 - loss: 2.1990 - val_accuracy: 0.2000 - val_loss: 2.2665\n",
            "Epoch 15/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2727 - loss: 2.1121 - val_accuracy: 0.1875 - val_loss: 2.2603\n",
            "Epoch 16/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2273 - loss: 2.1376 - val_accuracy: 0.2125 - val_loss: 2.2548\n",
            "Epoch 17/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1960 - loss: 2.1675 - val_accuracy: 0.2250 - val_loss: 2.2503\n",
            "Epoch 18/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.3037 - loss: 2.0857 - val_accuracy: 0.2125 - val_loss: 2.2538\n",
            "Epoch 19/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2516 - loss: 2.1184 - val_accuracy: 0.2125 - val_loss: 2.2524\n",
            "Epoch 20/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2008 - loss: 2.0994 - val_accuracy: 0.2000 - val_loss: 2.2485\n",
            "Epoch 21/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2768 - loss: 2.1114 - val_accuracy: 0.2000 - val_loss: 2.2496\n",
            "Epoch 22/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2534 - loss: 2.1248 - val_accuracy: 0.2000 - val_loss: 2.2497\n",
            "Epoch 23/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2905 - loss: 2.0892 - val_accuracy: 0.1625 - val_loss: 2.2449\n",
            "Epoch 24/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2459 - loss: 2.0952 - val_accuracy: 0.1750 - val_loss: 2.2375\n",
            "Epoch 25/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.2557 - loss: 2.1076 - val_accuracy: 0.1750 - val_loss: 2.2476\n",
            "Epoch 26/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.2431 - loss: 2.1021 - val_accuracy: 0.1625 - val_loss: 2.2566\n",
            "Epoch 27/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2751 - loss: 2.0790 - val_accuracy: 0.1875 - val_loss: 2.2628\n",
            "Epoch 28/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2441 - loss: 2.1000 - val_accuracy: 0.1750 - val_loss: 2.2670\n",
            "Epoch 29/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.3164 - loss: 2.0152 - val_accuracy: 0.1875 - val_loss: 2.2673\n",
            "Epoch 30/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2627 - loss: 2.0631 - val_accuracy: 0.1875 - val_loss: 2.2526\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "NBX9ihiCXs9f",
        "outputId": "8cc2877e-917a-4592-a2cf-56eff2e68240"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m429\u001b[0m, \u001b[38;5;34m32\u001b[0m)   ‚îÇ           \u001b[38;5;34m320\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m429\u001b[0m, \u001b[38;5;34m32\u001b[0m)   ‚îÇ           \u001b[38;5;34m128\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m214\u001b[0m, \u001b[38;5;34m32\u001b[0m)    ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m212\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ        \u001b[38;5;34m18,496\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_1           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m212\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ           \u001b[38;5;34m256\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m104\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ        \u001b[38;5;34m73,856\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_2           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m104\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ           \u001b[38;5;34m512\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ global_average_pooling2d        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m16,512\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ         \u001b[38;5;34m1,290\u001b[0m ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">429</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">429</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">214</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">212</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_1           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">212</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_2           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ global_average_pooling2d        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m333,216\u001b[0m (1.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">333,216</span> (1.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,922\u001b[0m (433.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,922</span> (433.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m221,846\u001b[0m (866.59 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">221,846</span> (866.59 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# IMPROVED CNN - RESEARCH-GRADE FIXES\n",
        "# =================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# =================================================================\n",
        "# PARAMETERS\n",
        "# =================================================================\n",
        "SAMPLE_RATE = 22050\n",
        "N_MELS = 128\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "SEGMENT_DURATION = 30  # 30-second segments (not full 60s)\n",
        "SEGMENTS_PER_SONG = 3  # Extract 3 random segments per song\n",
        "\n",
        "audio_base = '/content/drive/MyDrive/emotify_colab/data/audio/emotify'\n",
        "labels_path = '/content/drive/MyDrive/emotify_colab/data/emotify_labels_matched.csv'\n",
        "\n",
        "# =================================================================\n",
        "# FIX 1: SEGMENT-LEVEL EXTRACTION (BIGGEST FIX!)\n",
        "# =================================================================\n",
        "def extract_random_segment(audio_path, segment_duration=30):\n",
        "    \"\"\"Extract a random segment from the audio\"\"\"\n",
        "    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=60)\n",
        "\n",
        "    segment_len = int(SAMPLE_RATE * segment_duration)\n",
        "\n",
        "    # Random crop if long enough\n",
        "    if len(y) > segment_len:\n",
        "        start = np.random.randint(0, len(y) - segment_len)\n",
        "        y = y[start:start + segment_len]\n",
        "    else:\n",
        "        y = np.pad(y, (0, segment_len - len(y)))\n",
        "\n",
        "    # Extract mel-spectrogram\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr,\n",
        "        n_fft=N_FFT,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "    logmel = librosa.power_to_db(mel)\n",
        "    logmel = (logmel - np.mean(logmel)) / (np.std(logmel) + 1e-8)\n",
        "\n",
        "    return logmel\n",
        "\n",
        "# =================================================================\n",
        "# LOAD DATA WITH MULTIPLE SEGMENTS PER SONG\n",
        "# =================================================================\n",
        "print(\"Loading data with segment-level extraction...\")\n",
        "labels_df = pd.read_csv(labels_path)\n",
        "\n",
        "X_specs = []\n",
        "y_labels = []\n",
        "\n",
        "for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Extracting segments\"):\n",
        "    audio_path = os.path.join(audio_base, row['filename'])\n",
        "\n",
        "    if not os.path.exists(audio_path):\n",
        "        continue\n",
        "\n",
        "    # Extract multiple segments from each song\n",
        "    for _ in range(SEGMENTS_PER_SONG):\n",
        "        spec = extract_random_segment(audio_path, SEGMENT_DURATION)\n",
        "        X_specs.append(spec)\n",
        "        y_labels.append(row['dominant_emotion'])\n",
        "\n",
        "X = np.array(X_specs)[..., np.newaxis]\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_labels)\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"X shape: {X.shape}\")  # Should be ~1200 samples now!\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Classes: {len(np.unique(y))}\")\n",
        "print(f\"Data multiplication: {len(X) / len(labels_df):.1f}x\")\n",
        "\n",
        "# =================================================================\n",
        "# CNN ARCHITECTURE (keeping GAP - it's actually good!)\n",
        "# =================================================================\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        # Block 1\n",
        "        layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 2\n",
        "        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 3\n",
        "        layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Global pooling + Dense\n",
        "        layers.GlobalAveragePooling2D(),  # Keeping this!\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# =================================================================\n",
        "# FIX 2 & 4: 10-FOLD CV WITH CLASS WEIGHTS & ROC-AUC\n",
        "# =================================================================\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "fold_aucs = []\n",
        "fold_f1s = []\n",
        "fold_accs = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING IMPROVED CNN (Segment-level + Class Weights)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "    print(f\"\\nFold {fold}/10\")\n",
        "\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # FIX 2: Compute class weights\n",
        "    class_weights_array = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weights = dict(enumerate(class_weights_array))\n",
        "\n",
        "    # Build model\n",
        "    model = build_cnn(X.shape[1:], len(np.unique(y)))\n",
        "\n",
        "    # Callbacks\n",
        "    early_stop = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-6,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train with class weights\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        class_weight=class_weights,  # FIX 2: Class weights!\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # FIX 4: Evaluate with ROC-AUC!\n",
        "    y_pred = model.predict(X_val, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_val, y_pred_classes)\n",
        "    f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
        "\n",
        "    # ROC-AUC (proper comparison with ML models)\n",
        "    y_val_bin = label_binarize(y_val, classes=range(len(le.classes_)))\n",
        "    auc = roc_auc_score(y_val_bin, y_pred, average='macro', multi_class='ovr')\n",
        "\n",
        "    fold_accs.append(acc)\n",
        "    fold_f1s.append(f1)\n",
        "    fold_aucs.append(auc)\n",
        "\n",
        "    print(f\"Acc: {acc:.4f} | AUC: {auc:.4f} | F1: {f1:.4f} | Epochs: {len(history.history['loss'])}\")\n",
        "\n",
        "# =================================================================\n",
        "# FINAL RESULTS\n",
        "# =================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IMPROVED CNN RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {np.mean(fold_accs):.4f} ¬± {np.std(fold_accs):.4f}\")\n",
        "print(f\"ROC-AUC:   {np.mean(fold_aucs):.4f} ¬± {np.std(fold_aucs):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(fold_f1s):.4f} ¬± {np.std(fold_f1s):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON WITH BASELINES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"KNN:              0.52 ROC-AUC\")\n",
        "print(f\"SVM:              0.53 ROC-AUC\")\n",
        "print(f\"MLP:              0.57 ROC-AUC\")\n",
        "print(f\"Deep MLP:         0.58 ROC-AUC\")\n",
        "print(f\"CNN (improved):   {np.mean(fold_aucs):.4f} ROC-AUC\")\n",
        "\n",
        "if np.mean(fold_aucs) > 0.58:\n",
        "    improvement = ((np.mean(fold_aucs) - 0.58) / 0.58) * 100\n",
        "    print(f\"\\nüéâ CNN beats best baseline by +{improvement:.1f}%!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu_qNGqN2CBh",
        "outputId": "69c41ab5-7e0c-4110-afb0-3d37e6b97d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data with segment-level extraction...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 399/399 [04:41<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data loaded:\n",
            "X shape: (1197, 128, 1292, 1)\n",
            "y shape: (1197,)\n",
            "Classes: 10\n",
            "Data multiplication: 3.0x\n",
            "\n",
            "============================================================\n",
            "TRAINING IMPROVED CNN (Segment-level + Class Weights)\n",
            "============================================================\n",
            "\n",
            "Fold 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1667 | AUC: 0.5527 | F1: 0.0288 | Epochs: 17\n",
            "\n",
            "Fold 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1333 | AUC: 0.5626 | F1: 0.0439 | Epochs: 16\n",
            "\n",
            "Fold 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1d16ba25c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1d16ba25c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1083 | AUC: 0.5127 | F1: 0.0263 | Epochs: 16\n",
            "\n",
            "Fold 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1333 | AUC: 0.5072 | F1: 0.0246 | Epochs: 16\n",
            "\n",
            "Fold 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.0917 | AUC: 0.5363 | F1: 0.0392 | Epochs: 16\n",
            "\n",
            "Fold 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1333 | AUC: 0.5014 | F1: 0.0235 | Epochs: 17\n",
            "\n",
            "Fold 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1083 | AUC: 0.5347 | F1: 0.0516 | Epochs: 16\n",
            "\n",
            "Fold 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1092 | AUC: 0.5068 | F1: 0.0350 | Epochs: 16\n",
            "\n",
            "Fold 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1345 | AUC: 0.5239 | F1: 0.0375 | Epochs: 17\n",
            "\n",
            "Fold 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.1176 | AUC: 0.5104 | F1: 0.0212 | Epochs: 16\n",
            "\n",
            "============================================================\n",
            "IMPROVED CNN RESULTS\n",
            "============================================================\n",
            "Accuracy:  0.1236 ¬± 0.0200\n",
            "ROC-AUC:   0.5249 ¬± 0.0199\n",
            "F1-Score:  0.0332 ¬± 0.0094\n",
            "\n",
            "============================================================\n",
            "COMPARISON WITH BASELINES\n",
            "============================================================\n",
            "KNN:              0.52 ROC-AUC\n",
            "SVM:              0.53 ROC-AUC\n",
            "MLP:              0.57 ROC-AUC\n",
            "Deep MLP:         0.58 ROC-AUC\n",
            "CNN (improved):   0.5249 ROC-AUC\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "x4MG5HVQCxHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# LSTM FOR EMOTIFY+ MER\n",
        "# =================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# =================================================================\n",
        "# PARAMETERS\n",
        "# =================================================================\n",
        "SAMPLE_RATE = 22050\n",
        "N_MELS = 128\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "MAX_DURATION = 30  # 30 seconds\n",
        "\n",
        "audio_base = '/content/drive/MyDrive/emotify_colab/data/audio/emotify'\n",
        "labels_path = '/content/drive/MyDrive/emotify_colab/data/emotify_labels_matched.csv'\n",
        "\n",
        "# =================================================================\n",
        "# FEATURE EXTRACTION FOR LSTM (TIME SERIES)\n",
        "# =================================================================\n",
        "def extract_mel_sequence(audio_path, duration=30):\n",
        "    \"\"\"Extract mel-spectrogram as a time sequence\"\"\"\n",
        "    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=60)\n",
        "\n",
        "    segment_len = int(SAMPLE_RATE * duration)\n",
        "\n",
        "    # Random crop\n",
        "    if len(y) > segment_len:\n",
        "        start = np.random.randint(0, len(y) - segment_len)\n",
        "        y = y[start:start + segment_len]\n",
        "    else:\n",
        "        y = np.pad(y, (0, segment_len - len(y)))\n",
        "\n",
        "    # Extract mel-spectrogram\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr,\n",
        "        n_fft=N_FFT,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "    logmel = librosa.power_to_db(mel)\n",
        "\n",
        "    # Normalize per feature (across time)\n",
        "    logmel = (logmel - np.mean(logmel, axis=1, keepdims=True)) / (np.std(logmel, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "    # Transpose: (time_steps, features)\n",
        "    logmel = logmel.T  # Shape: (time_steps, 128)\n",
        "\n",
        "    return logmel\n",
        "\n",
        "# =================================================================\n",
        "# LOAD DATA WITH SEGMENTS\n",
        "# =================================================================\n",
        "print(\"Extracting mel sequences for LSTM...\")\n",
        "labels_df = pd.read_csv(labels_path)\n",
        "\n",
        "SEGMENTS_PER_SONG = 3  # 3 segments per song\n",
        "\n",
        "X_seqs = []\n",
        "y_labels = []\n",
        "\n",
        "for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Extracting sequences\"):\n",
        "    audio_path = os.path.join(audio_base, row['filename'])\n",
        "\n",
        "    if not os.path.exists(audio_path):\n",
        "        continue\n",
        "\n",
        "    # Extract multiple segments\n",
        "    for _ in range(SEGMENTS_PER_SONG):\n",
        "        seq = extract_mel_sequence(audio_path, MAX_DURATION)\n",
        "        X_seqs.append(seq)\n",
        "        y_labels.append(row['dominant_emotion'])\n",
        "\n",
        "X = np.array(X_seqs)  # Shape: (N, time_steps, 128)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_labels)\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"X shape: {X.shape}\")  # Should be (1197, time_steps, 128)\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Classes: {len(np.unique(y))}\")\n",
        "\n",
        "# =================================================================\n",
        "# LSTM MODEL ARCHITECTURES\n",
        "# =================================================================\n",
        "\n",
        "def build_lstm(input_shape, num_classes):\n",
        "    \"\"\"Simple LSTM\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "\n",
        "        layers.LSTM(128, return_sequences=True),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.LSTM(128, return_sequences=False),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_bilstm(input_shape, num_classes):\n",
        "    \"\"\"Bidirectional LSTM\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "\n",
        "        layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Bidirectional(layers.LSTM(64, return_sequences=False)),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_gru(input_shape, num_classes):\n",
        "    \"\"\"GRU (faster alternative to LSTM)\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "\n",
        "        layers.GRU(128, return_sequences=True),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.GRU(128, return_sequences=False),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# =================================================================\n",
        "# TRAIN ALL RNN MODELS WITH 10-FOLD CV\n",
        "# =================================================================\n",
        "\n",
        "models_to_train = {\n",
        "    'LSTM': build_lstm,\n",
        "    'BiLSTM': build_bilstm,\n",
        "    'GRU': build_gru\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for model_name, model_builder in models_to_train.items():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"TRAINING {model_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_aucs = []\n",
        "    fold_f1s = []\n",
        "    fold_accs = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"\\nFold {fold}/10\", end=\" \")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Class weights\n",
        "        class_weights_array = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.unique(y_train),\n",
        "            y=y_train\n",
        "        )\n",
        "        class_weights = dict(enumerate(class_weights_array))\n",
        "\n",
        "        # Build model\n",
        "        model = model_builder(X.shape[1:], len(np.unique(y)))\n",
        "\n",
        "        # Callbacks\n",
        "        early_stop = callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=20,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=7,\n",
        "            min_lr=1e-6,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = model.predict(X_val, verbose=0)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred_classes)\n",
        "        f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
        "\n",
        "        # ROC-AUC\n",
        "        y_val_bin = label_binarize(y_val, classes=range(len(le.classes_)))\n",
        "        auc = roc_auc_score(y_val_bin, y_pred, average='macro', multi_class='ovr')\n",
        "\n",
        "        fold_accs.append(acc)\n",
        "        fold_f1s.append(f1)\n",
        "        fold_aucs.append(auc)\n",
        "\n",
        "        print(f\"| Acc: {acc:.4f} | AUC: {auc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    all_results[model_name] = {\n",
        "        'accuracy': (np.mean(fold_accs), np.std(fold_accs)),\n",
        "        'roc_auc': (np.mean(fold_aucs), np.std(fold_aucs)),\n",
        "        'f1': (np.mean(fold_f1s), np.std(fold_f1s))\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  Accuracy:  {np.mean(fold_accs):.4f} ¬± {np.std(fold_accs):.4f}\")\n",
        "    print(f\"  ROC-AUC:   {np.mean(fold_aucs):.4f} ¬± {np.std(fold_aucs):.4f}\")\n",
        "    print(f\"  F1-Score:  {np.mean(fold_f1s):.4f} ¬± {np.std(fold_f1s):.4f}\")\n",
        "\n",
        "# =================================================================\n",
        "# FINAL SUMMARY\n",
        "# =================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPLETE RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nML Baselines:\")\n",
        "print(\"  KNN:              0.5200 ROC-AUC\")\n",
        "print(\"  SVM:              0.5300 ROC-AUC\")\n",
        "print(\"  Extra Trees:      0.5400 ROC-AUC\")\n",
        "print(\"  MLP:              0.5700 ROC-AUC\")\n",
        "print(\"  Deep MLP:         0.5800 ROC-AUC\")\n",
        "\n",
        "print(\"\\nDeep Learning:\")\n",
        "print(f\"  CNN:              0.5249 ROC-AUC\")\n",
        "\n",
        "for model_name, results in all_results.items():\n",
        "    auc_mean, auc_std = results['roc_auc']\n",
        "    print(f\"  {model_name:17s} {auc_mean:.4f} ¬± {auc_std:.4f} ROC-AUC\")\n",
        "\n",
        "# Find best model\n",
        "best_model = max(all_results.items(), key=lambda x: x[1]['roc_auc'][0])\n",
        "best_auc = best_model[1]['roc_auc'][0]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"BEST MODEL: {best_model[0]} with {best_auc:.4f} ROC-AUC\")\n",
        "\n",
        "if best_auc > 0.58:\n",
        "    improvement = ((best_auc - 0.58) / 0.58) * 100\n",
        "    print(f\"üéâ Improvement over Deep MLP: +{improvement:.1f}%\")\n",
        "else:\n",
        "    print(\"Note: RNNs perform similarly to feature-based models\")\n",
        "    print(\"This confirms that on small datasets, hand-crafted features remain competitive\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame([\n",
        "    {'Model': k,\n",
        "     'ROC_AUC': f\"{v['roc_auc'][0]:.4f} ¬± {v['roc_auc'][1]:.4f}\",\n",
        "     'F1': f\"{v['f1'][0]:.4f} ¬± {v['f1'][1]:.4f}\"}\n",
        "    for k, v in all_results.items()\n",
        "])\n",
        "results_df.to_csv('rnn_results.csv', index=False)\n",
        "print(\"\\n‚úì Results saved to: rnn_results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMFbrxR6C0pg",
        "outputId": "317a5611-cc53-4c5d-f371-7708f301eafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mel sequences for LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 399/399 [13:20<00:00,  2.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data loaded:\n",
            "X shape: (1197, 1292, 128)\n",
            "y shape: (1197,)\n",
            "Classes: 10\n",
            "\n",
            "============================================================\n",
            "TRAINING LSTM\n",
            "============================================================\n",
            "\n",
            "Fold 1/10 | Acc: 0.1000 | AUC: 0.5701 | F1: 0.0838\n",
            "\n",
            "Fold 2/10 | Acc: 0.1083 | AUC: 0.5674 | F1: 0.0828\n",
            "\n",
            "Fold 3/10 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79d81dd92e80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79d81dd92e80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Acc: 0.1917 | AUC: 0.5698 | F1: 0.1729\n",
            "\n",
            "Fold 4/10 | Acc: 0.1000 | AUC: 0.4965 | F1: 0.0795\n",
            "\n",
            "Fold 5/10 | Acc: 0.1500 | AUC: 0.5636 | F1: 0.1613\n",
            "\n",
            "Fold 6/10 | Acc: 0.1417 | AUC: 0.5332 | F1: 0.1140\n",
            "\n",
            "Fold 7/10 | Acc: 0.1167 | AUC: 0.5538 | F1: 0.1111\n",
            "\n",
            "Fold 8/10 | Acc: 0.1176 | AUC: 0.4886 | F1: 0.1147\n",
            "\n",
            "Fold 9/10 | Acc: 0.1176 | AUC: 0.5797 | F1: 0.1015\n",
            "\n",
            "Fold 10/10 | Acc: 0.1008 | AUC: 0.5654 | F1: 0.0863\n",
            "\n",
            "LSTM Results:\n",
            "  Accuracy:  0.1244 ¬± 0.0276\n",
            "  ROC-AUC:   0.5488 ¬± 0.0305\n",
            "  F1-Score:  0.1108 ¬± 0.0310\n",
            "\n",
            "============================================================\n",
            "TRAINING BiLSTM\n",
            "============================================================\n",
            "\n",
            "Fold 1/10 | Acc: 0.1333 | AUC: 0.5630 | F1: 0.1110\n",
            "\n",
            "Fold 2/10 | Acc: 0.1250 | AUC: 0.5378 | F1: 0.1196\n",
            "\n",
            "Fold 3/10 | Acc: 0.0750 | AUC: 0.5515 | F1: 0.0709\n",
            "\n",
            "Fold 4/10 | Acc: 0.1500 | AUC: 0.5777 | F1: 0.1264\n",
            "\n",
            "Fold 5/10 | Acc: 0.1417 | AUC: 0.5994 | F1: 0.1458\n",
            "\n",
            "Fold 6/10 | Acc: 0.1167 | AUC: 0.5295 | F1: 0.0848\n",
            "\n",
            "Fold 7/10 | Acc: 0.1000 | AUC: 0.5144 | F1: 0.0978\n",
            "\n",
            "Fold 8/10 | Acc: 0.1008 | AUC: 0.5237 | F1: 0.0884\n",
            "\n",
            "Fold 9/10 | Acc: 0.0672 | AUC: 0.5291 | F1: 0.0514\n",
            "\n",
            "Fold 10/10 | Acc: 0.0840 | AUC: 0.5243 | F1: 0.0792\n",
            "\n",
            "BiLSTM Results:\n",
            "  Accuracy:  0.1094 ¬± 0.0271\n",
            "  ROC-AUC:   0.5450 ¬± 0.0260\n",
            "  F1-Score:  0.0975 ¬± 0.0269\n",
            "\n",
            "============================================================\n",
            "TRAINING GRU\n",
            "============================================================\n",
            "\n",
            "Fold 1/10 | Acc: 0.1167 | AUC: 0.5028 | F1: 0.1005\n",
            "\n",
            "Fold 2/10 | Acc: 0.1417 | AUC: 0.5649 | F1: 0.1271\n",
            "\n",
            "Fold 3/10 | Acc: 0.1583 | AUC: 0.5642 | F1: 0.1122\n",
            "\n",
            "Fold 4/10 | Acc: 0.1417 | AUC: 0.5253 | F1: 0.1092\n",
            "\n",
            "Fold 5/10 | Acc: 0.2000 | AUC: 0.5917 | F1: 0.1764\n",
            "\n",
            "Fold 6/10 | Acc: 0.1250 | AUC: 0.5471 | F1: 0.0966\n",
            "\n",
            "Fold 7/10 | Acc: 0.2000 | AUC: 0.5662 | F1: 0.1761\n",
            "\n",
            "Fold 8/10 | Acc: 0.1008 | AUC: 0.5284 | F1: 0.0850\n",
            "\n",
            "Fold 9/10 | Acc: 0.0924 | AUC: 0.5629 | F1: 0.1000\n",
            "\n",
            "Fold 10/10 | Acc: 0.1176 | AUC: 0.5801 | F1: 0.1160\n",
            "\n",
            "GRU Results:\n",
            "  Accuracy:  0.1394 ¬± 0.0355\n",
            "  ROC-AUC:   0.5534 ¬± 0.0259\n",
            "  F1-Score:  0.1199 ¬± 0.0302\n",
            "\n",
            "============================================================\n",
            "COMPLETE RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "ML Baselines:\n",
            "  KNN:              0.5200 ROC-AUC\n",
            "  SVM:              0.5300 ROC-AUC\n",
            "  Extra Trees:      0.5400 ROC-AUC\n",
            "  MLP:              0.5700 ROC-AUC\n",
            "  Deep MLP:         0.5800 ROC-AUC\n",
            "\n",
            "Deep Learning:\n",
            "  CNN:              0.5249 ROC-AUC\n",
            "  LSTM              0.5488 ¬± 0.0305 ROC-AUC\n",
            "  BiLSTM            0.5450 ¬± 0.0260 ROC-AUC\n",
            "  GRU               0.5534 ¬± 0.0259 ROC-AUC\n",
            "\n",
            "============================================================\n",
            "BEST MODEL: GRU with 0.5534 ROC-AUC\n",
            "Note: RNNs perform similarly to feature-based models\n",
            "This confirms that on small datasets, hand-crafted features remain competitive\n",
            "============================================================\n",
            "\n",
            "‚úì Results saved to: rnn_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrWnQVsVC3Bh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPZVRubkdjevzEPe6c/f7Rd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}